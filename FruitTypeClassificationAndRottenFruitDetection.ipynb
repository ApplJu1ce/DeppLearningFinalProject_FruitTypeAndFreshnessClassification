{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PLQr9MxjekC",
        "outputId": "e93bc1c9-a87d-4c34-bb5c-137c649f1934"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'fruits-fresh-and-rotten-for-classification' dataset.\n",
            "Path to dataset files: /kaggle/input/fruits-fresh-and-rotten-for-classification\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"sriramr/fruits-fresh-and-rotten-for-classification\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "\n",
        "root_dir = os.path.join(path, \"dataset\")\n",
        "train_dir = os.path.join(root_dir, \"train\")\n",
        "test_dir = os.path.join(root_dir, \"test\")"
      ],
      "metadata": {
        "id": "MxoR6nvKBMV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "path = \"/root/.cache/kagglehub/datasets/sriramr/fruits-fresh-and-rotten-for-classification/versions/1/dataset/train/freshapples\"\n",
        "for i, f in enumerate(os.listdir(path)[:5]):\n",
        "    img = Image.open(os.path.join(path, f))\n",
        "    print(img.size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2l0IoHPQGelD",
        "outputId": "7d6dc90a-11da-46d6-ff14-eac7a1570caf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(390, 418)\n",
            "(438, 372)\n",
            "(426, 422)\n",
            "(390, 360)\n",
            "(336, 374)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform_train)\n",
        "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2, pin_memory=True, persistent_workers=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False, num_workers=2, pin_memory=True, persistent_workers=True)\n",
        "\n",
        "print(\"Classes:\", train_dataset.classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNF8VJZVFL94",
        "outputId": "41cfc992-3e99-40b1-9976-94e152646bd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['freshapples', 'freshbanana', 'freshoranges', 'rottenapples', 'rottenbanana', 'rottenoranges']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Compact CNN + Training, Plots, Confusion, TP/FP\n",
        "# =========================\n",
        "import torch, math, time\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.backends.cudnn.benchmark = True  # speed on fixed input size (128x128)\n",
        "\n",
        "# ---- classes from your dataset ----\n",
        "classes = train_dataset.classes\n",
        "num_classes = len(classes)\n",
        "print(\"Detected classes:\", classes)\n",
        "\n",
        "# ---- Model: light but accurate ----\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, k=3, s=1, p=1, drop=0.0):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, k, s, p, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_ch)\n",
        "        self.act = nn.SiLU(inplace=True)\n",
        "        self.drop = nn.Dropout2d(drop) if drop > 0 else nn.Identity()\n",
        "    def forward(self, x):\n",
        "        x = self.act(self.bn(self.conv(x)))\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "class SmallFruitCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    ~1.3M params, fast on Colab free tier.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=6, drop=0.10):\n",
        "        super().__init__()\n",
        "        # 3x128x128\n",
        "        self.stem = ConvBlock(3, 32, k=3, s=1, p=1, drop=0.0)          # -> 32x128x128\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.MaxPool2d(2),                                           # -> 32x64x64\n",
        "            ConvBlock(32, 64, drop=drop),                              # -> 64x64x64\n",
        "            ConvBlock(64, 64, drop=drop),                              # -> 64x64x64\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.MaxPool2d(2),                                           # -> 64x32x32\n",
        "            ConvBlock(64, 128, drop=drop),                             # -> 128x32x32\n",
        "            ConvBlock(128, 128, drop=drop),                            # -> 128x32x32\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.MaxPool2d(2),                                           # -> 128x16x16\n",
        "            ConvBlock(128, 256, drop=drop),                            # -> 256x16x16\n",
        "            ConvBlock(256, 256, drop=drop),                            # -> 256x16x16\n",
        "        )\n",
        "        self.head = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),                                   # -> 256x1x1\n",
        "            nn.Flatten(),                                              # -> 256\n",
        "            nn.Dropout(0.20),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.SiLU(inplace=True),\n",
        "            nn.Dropout(0.20),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "# ---- Config ----\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    epochs:int = 15\n",
        "    lr:float = 2e-3\n",
        "    weight_decay:float = 1e-4\n",
        "    label_smoothing:float = 0.00   # helps generalization\n",
        "    grad_clip:float = 1.0          # set 0 or None to disable\n",
        "    report_every:int = 1\n",
        "    save_best:bool = True\n",
        "\n",
        "cfg = TrainConfig()\n",
        "\n",
        "# ---- Utilities ----\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device, criterion):\n",
        "    model.eval()\n",
        "    total_loss, total_correct, n = 0.0, 0, 0\n",
        "    all_preds, all_targets = [], []\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        bs = y.size(0)\n",
        "        total_loss += loss.item() * bs\n",
        "        preds = logits.argmax(1)\n",
        "        total_correct += (preds == y).sum().item()\n",
        "        n += bs\n",
        "        all_preds.append(preds.detach().cpu())\n",
        "        all_targets.append(y.detach().cpu())\n",
        "    loss = total_loss / max(1, n)\n",
        "    acc = total_correct / max(1, n)\n",
        "    all_preds = torch.cat(all_preds).numpy()\n",
        "    all_targets = torch.cat(all_targets).numpy()\n",
        "    return loss, acc, all_preds, all_targets\n",
        "\n",
        "def plot_history(history):\n",
        "    # Loss\n",
        "    plt.figure()\n",
        "    plt.plot(history[\"train_loss\"], label=\"train_loss\")\n",
        "    plt.plot(history[\"val_loss\"], label=\"val_loss\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Loss over epochs\"); plt.legend(); plt.show()\n",
        "\n",
        "    # Accuracy\n",
        "    plt.figure()\n",
        "    plt.plot(history[\"train_acc\"], label=\"train_acc\")\n",
        "    plt.plot(history[\"val_acc\"], label=\"val_acc\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.title(\"Accuracy over epochs\"); plt.legend(); plt.show()\n",
        "\n",
        "def plot_confusion(y_true, y_pred, class_names, normalize=None, title=\"Confusion Matrix\"):\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(class_names))), normalize=normalize)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "    plt.figure(figsize=(6,5))\n",
        "    disp.plot(include_values=True, xticks_rotation=45, colorbar=True)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "    return cm\n",
        "\n",
        "def plot_tp_fp_only(cm, class_names):\n",
        "    \"\"\"\n",
        "    Given a standard (unnormalized) confusion matrix `cm`,\n",
        "    build a 2 x C matrix containing TP (diag) and FP (column sum - TP) for each class,\n",
        "    then render as a small heatmap-like matrix and a bar chart.\n",
        "    \"\"\"\n",
        "    cm = np.asarray(cm)\n",
        "    col_sums = cm.sum(axis=0)  # predicted totals per class\n",
        "    tp = np.diag(cm)\n",
        "    fp = col_sums - tp\n",
        "\n",
        "    tp_fp = np.vstack([tp, fp])\n",
        "\n",
        "    # Heatmap-like display\n",
        "    fig, ax = plt.subplots(figsize=(8, 2.8))\n",
        "    im = ax.imshow(tp_fp, aspect=\"auto\")\n",
        "    ax.set_yticks([0, 1]); ax.set_yticklabels([\"TP\", \"FP\"])\n",
        "    ax.set_xticks(np.arange(len(class_names))); ax.set_xticklabels(class_names, rotation=45, ha=\"right\")\n",
        "    for i in range(2):\n",
        "        for j in range(len(class_names)):\n",
        "            ax.text(j, i, int(tp_fp[i, j]), ha=\"center\", va=\"center\", color=\"w\" if tp_fp[i, j] > tp_fp.max()/2 else \"black\")\n",
        "    ax.set_title(\"TP/FP per class (counts)\")\n",
        "    fig.colorbar(im, ax=ax)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Bar plot\n",
        "    x = np.arange(len(class_names))\n",
        "    width = 0.35\n",
        "    plt.figure(figsize=(8, 3))\n",
        "    plt.bar(x - width/2, tp, width, label=\"TP\")\n",
        "    plt.bar(x + width/2, fp, width, label=\"FP\")\n",
        "    plt.xticks(x, class_names, rotation=45, ha=\"right\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.title(\"Per-class TP and FP\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return tp, fp\n",
        "\n",
        "# ---- Train ----\n",
        "def train(model, train_loader, val_loader, device, cfg):\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=cfg.label_smoothing)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "    # Lightweight cosine schedule\n",
        "    total_steps = cfg.epochs * len(train_loader)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps)\n",
        "\n",
        "    scaler = torch.amp.GradScaler(enabled=(device.type == \"cuda\"))\n",
        "\n",
        "    history = {\"train_loss\":[], \"val_loss\":[], \"train_acc\":[], \"val_acc\":[], \"lr\":[]}\n",
        "    best_acc, best_state = 0.0, None\n",
        "\n",
        "    global_step = 0\n",
        "    for epoch in range(1, cfg.epochs+1):\n",
        "        model.train()\n",
        "        run_loss, run_correct, n = 0.0, 0, 0\n",
        "\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with torch.amp.autocast(device_type=device.type, dtype=torch.float16, enabled=(device.type == \"cuda\")):\n",
        "                logits = model(xb)\n",
        "                loss = criterion(logits, yb)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            if cfg.grad_clip and cfg.grad_clip > 0:\n",
        "                scaler.unscale_(optimizer)\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            scheduler.step()\n",
        "            global_step += 1\n",
        "\n",
        "            bs = yb.size(0)\n",
        "            run_loss += loss.item() * bs\n",
        "            run_correct += (logits.argmax(1) == yb).sum().item()\n",
        "            n += bs\n",
        "            history[\"lr\"].append(optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "        train_loss = run_loss / max(1, n)\n",
        "        train_acc = run_correct / max(1, n)\n",
        "\n",
        "        val_loss, val_acc, _, _ = evaluate(model, val_loader, device, criterion)\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "        with torch.no_grad():\n",
        "          model.eval()\n",
        "          train_eval_loss, train_eval_acc, _, _ = evaluate(model, train_loader, device, criterion)\n",
        "\n",
        "        if epoch % cfg.report_every == 0:\n",
        "            print(f\"[Epoch {epoch:02d}] \"\n",
        "                  f\"train_mode_acc {train_acc:.3f} | eval(train)_acc {train_eval_acc:.3f} | \"\n",
        "                  f\"test_acc {val_acc:.3f} | \"\n",
        "                  f\"train_loss {train_loss:.3f} | eval(train)_loss {train_eval_loss:.3f} | test_loss {val_loss:.3f}\")\n",
        "\n",
        "        if cfg.save_best and val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
        "\n",
        "    if cfg.save_best and best_state is not None:\n",
        "        model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n",
        "        print(f\"Loaded best checkpoint (val_acc={best_acc:.4f})\")\n",
        "\n",
        "    return model, history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17kCOYlfOv1T",
        "outputId": "30c1c503-74e2-4e02-9b02-974654caed6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected classes: ['freshapples', 'freshbanana', 'freshoranges', 'rottenapples', 'rottenbanana', 'rottenoranges']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Run training ----\n",
        "model = SmallFruitCNN(num_classes=num_classes, drop=0.10)\n",
        "model, history = train(model, train_loader, test_loader, device, cfg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lS9ajtpvasT",
        "outputId": "64d15adf-8b9b-4483-ba7f-932d0597ecba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 01] train_mode_acc 0.752 | eval(train)_acc 0.848 | test_acc 0.850 | train_loss 0.699 | eval(train)_loss 0.409 | test_loss 0.406\n",
            "[Epoch 02] train_mode_acc 0.840 | eval(train)_acc 0.855 | test_acc 0.854 | train_loss 0.455 | eval(train)_loss 0.422 | test_loss 0.423\n",
            "[Epoch 03] train_mode_acc 0.864 | eval(train)_acc 0.912 | test_acc 0.914 | train_loss 0.395 | eval(train)_loss 0.261 | test_loss 0.254\n",
            "[Epoch 04] train_mode_acc 0.884 | eval(train)_acc 0.906 | test_acc 0.909 | train_loss 0.339 | eval(train)_loss 0.258 | test_loss 0.256\n",
            "[Epoch 05] train_mode_acc 0.894 | eval(train)_acc 0.926 | test_acc 0.924 | train_loss 0.309 | eval(train)_loss 0.213 | test_loss 0.227\n",
            "[Epoch 06] train_mode_acc 0.902 | eval(train)_acc 0.944 | test_acc 0.946 | train_loss 0.279 | eval(train)_loss 0.153 | test_loss 0.151\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Plots: loss/acc ----\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "aA82-BTXvc1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Confusion matrices ----\n",
        "criterion_eval = nn.CrossEntropyLoss()  # for eval only\n",
        "test_loss, test_acc, preds, targets = evaluate(model, test_loader, device, criterion_eval)\n",
        "print(f\"Final test: loss={test_loss:.4f}, acc={test_acc:.4f}\")\n",
        "\n",
        "# 1) Raw confusion matrix (counts)\n",
        "cm_counts = plot_confusion(targets, preds, classes, normalize=None, title=\"Confusion Matrix (counts)\")\n",
        "\n",
        "# 2) Normalized by true labels (optional, easier to read recall per class)\n",
        "_ = plot_confusion(targets, preds, classes, normalize='true', title=\"Confusion Matrix (normalized by true)\")\n",
        "\n",
        "# 3) TP/FP-only \"confusion\"\n",
        "tp, fp = plot_tp_fp_only(cm_counts, classes)\n",
        "for name, tpi, fpi in zip(classes, tp, fp):\n",
        "    print(f\"{name:>15}: TP={int(tpi):4d}  FP={int(fpi):4d}\")\n"
      ],
      "metadata": {
        "id": "hfFY2CPnvfGO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}